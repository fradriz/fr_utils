#!usr/bin/python3
# -*- coding: utf-8 -*-

import re
import os
import math
import boto3
import urllib.request

from datetime import datetime

from botocore.exceptions import ClientError
from psutil import virtual_memory
from pyspark.sql import SparkSession
from subprocess import Popen, run, PIPE, CalledProcessError

from email import encoders
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email.mime.multipart import MIMEMultipart


def del_object(s3_res, bucket, obj):
    print('Deleting {}'.format(obj['Key']))
    s3_res.Object(bucket, obj['Key']).delete()


def clean_emr_metadata(dest):
    s3_client = boto3.client('s3')
    s3_res = boto3.resource('s3')

    bucket = dest.replace('s3://', '').split('/')[0]
    prefix = '/'.join(dest.replace('s3://', '').split('/')[1:-2])  # Exclude dt

    print('Deleting _$folder$ keys generated by EMR from bucket {} '
          'and prefix {}'.format(bucket, prefix))

    resp = s3_client.list_objects_v2(
        Bucket=bucket,
        Prefix=prefix,
        MaxKeys=1000
    )

    while resp['KeyCount'] > 0:
        [del_object(s3_res, bucket, obj)
         for obj in resp['Contents']
         if obj['Key'].endswith('_$folder$')]

        resp = s3_client.list_objects_v2(
            Bucket=bucket,
            Prefix=prefix,
            MaxKeys=1000,
            StartAfter=resp['Contents'][-1]['Key']
        )


def emr_write_dataframe(df, dest, num_parts, spark, repart=True):
    # WRITING TO HDFS AND THEN UPLOADING
    local_output = f'hdfs:///transformed/{dest}'

    if repart:
        print(f'Writing transformed DataFrame to {local_output}')
        df.repartition(num_parts).write \
            .mode('overwrite') \
            .parquet(local_output)
    else:
        print(f'Writing transformed DataFrame (without repartition) to {local_output}')
        df.write \
            .mode('overwrite') \
            .parquet(local_output)

    spark.stop()

    print('Copying data into S3 using s3-dist-cp...')
    copy_cmd = f'/usr/bin/s3-dist-cp --src {local_output} ' \
               f'--dest s3://{dest} --targetSize 128'
    print(f'Command: {copy_cmd}')
    os.system(copy_cmd)

    print('Removing data from HDFS...')
    rm_cmd = f'hdfs dfs -rm -r -f -skipTrash {local_output}'
    print('Command: {}'.format(rm_cmd))
    os.system(rm_cmd)

    clean_emr_metadata(f's3://{dest}')


def ec2_write_dataframe(df, dest, num_parts, spark):
    df.repartition(num_parts).write \
        .mode('overwrite') \
        .parquet(f'/mnt/{dest}')

    Popen(f'aws s3 cp /mnt/{dest} s3://{dest} --recursive'
          .split(), stdout=PIPE) \
        .communicate()[0].decode('utf-8')

    Popen(f'rm -r /mnt/{dest}'.split(), stdout=PIPE)

    spark.stop()


def create_ec2_spark_session():
    tot_mem = virtual_memory().total >> 30
    warehouse_location = os.path.abspath('spark-warehouse')

    spark = SparkSession.builder \
        .config("spark.sql.warehouse.dir", warehouse_location) \
        .config("spark.driver.memory", str(tot_mem - 5) + 'g') \
        .config("spark.network.timeout", "20000s") \
        .config("spark.debug.maxToStringFields", "1000") \
        .config("spark.executor.heartbeatInterval", '10000s') \
        .config('spark.hadoop.fs.s3a.connection.maximum', '64') \
        .config('spark.hadoop.fs.s3a.threads.max', '64') \
        .config('spark.hadoop.fs.s3a.block.size', '100000000') \
        .config('spark.hadoop.fs.s3a.max.total.tasks', '64') \
        .config('spark.hadoop.fs.s3a.multipart.size', '100000000') \
        .config('spark.hadoop.fs.s3a.multipart.threshold', '2147483647') \
        .config('spark.hadoop.fs.s3a.attempts.maximum', '20') \
        .config('spark.hadoop.fs.s3a.connection.establish.timeout', '500000') \
        .config('spark.hadoop.fs.s3a.connection.timeout', '200000') \
        .config('spark.hadoop.fs.s3a.paging.maximum', '500000') \
        .enableHiveSupport() \
        .getOrCreate()

    return spark


def create_emr_spark_session():
    spark = SparkSession.builder \
        .config("spark.network.timeout", "20000s") \
        .config('spark.hadoop.fs.s3a.multipart.size', '10485760') \
        .config('spark.hadoop.fs.s3a.fast.upload.default', 'true') \
        .config('spark.hadoop.fs.s3a.fast.upload', 'true') \
        .config('spark.hadoop.fs.s3a.threads.max', '136') \
        .config('spark.hadoop.fs.s3a.multipart.threshold', '104857600') \
        .config('spark.hadoop.fs.s3a.connection.maximum', '200') \
        .config('spark.hadoop.parquet.memory.pool.ratio', '0.5') \
        .config("spark.dynamicAllocation.enabled", "false") \
        .getOrCreate()

    return spark


def calculate_approx_partitions(s3_path, size=128e6):
    """
    Simple AWS util...will rewrite in boto3 at some point.
    :param s3_path: s3 path to get file sizes: eg. s3://bucket/prefix
    :param size: size in bytes of the approx block sizes you want.
    :return: integer to be set as df.repartition() size.
    """

    s3_path = os.path.join(s3_path, '')
    output = run(
        """aws s3 ls {s3_path}""".format(s3_path=s3_path),
        shell=True,
        stdout=PIPE
    ).stdout.decode().split("\n")
    output.pop()

    return math.ceil(sum([int(p.split()[2]) for p in output]) / size)


def return_ec2_region():
    avregex = r"^(us|eu|ap|sa|cn)-([a-zA-Z-]{4,10})-(\d)(\w{0,2})$"
    avz = urllib.request.urlopen("http://169.254.169.254/latest/meta-data/placement/availability-zone/").read().decode()
    return "-".join(re.search(avregex, avz).groups()[:3]) if re.match(avregex, avz) else None


def list_partitions_gi(bucket, prefix, delimiter='/'):
    """
    List date partitions of a S3 bucket with date partition format %Y/%m/%d
    Inputs:
          - bucket: S3 bucket name
          - prefix: S3 path after bucket name
          - delimiter: S3 path delimiter, by default '/'
    Output:
          - res: a list of date partitions
    """
    res = []
    bucket = bucket if not bucket.startswith(delimiter) else bucket[1:]
    prefix = prefix if prefix.endswith(delimiter) else prefix + delimiter
    prefix = prefix if not prefix.startswith(delimiter) else prefix[1:]
    conn = boto3.client('s3')
    s3_result = conn.list_objects_v2(Bucket=bucket, Delimiter=delimiter, Prefix=prefix)
    common_prefixes = s3_result.get('CommonPrefixes')
    if not common_prefixes:
        return []
    for i in common_prefixes:
        i_res = conn.list_objects_v2(Bucket=bucket, Delimiter=delimiter, Prefix=i["Prefix"]) \
            .get('CommonPrefixes')
        # print (i_res)
        for j in i_res:
            # print (j["Prefix"])
            j_res = conn.list_objects_v2(Bucket=bucket, Delimiter=delimiter, Prefix=j["Prefix"]) \
                .get('CommonPrefixes')
            for k in j_res:
                tmp = "dt=" + k['Prefix'].split(delimiter)[-4] \
                      + "-" + k['Prefix'].split(delimiter)[-3] \
                      + "-" + k['Prefix'].split(delimiter)[-2]
                res.append(tmp)
    return res


def list_partitions_dt(bucket, prefix, delimiter='/'):
    """
    List date partitions of a S3 bucket with date partition format dt=%Y-%m-%d
    Inputs:
          - bucket: S3 bucket name
          - prefix: S3 path after bucket name
          - delimiter: S3 path delimiter, by default '/'
    Output:
          - res: a list of date partitions
    """
    bucket = bucket if not bucket.startswith(delimiter) else bucket[1:]
    prefix = prefix if prefix.endswith(delimiter) else prefix + delimiter
    prefix = prefix if not prefix.startswith(delimiter) else prefix[1:]
    conn = boto3.client('s3')
    s3_result = conn.list_objects_v2(Bucket=bucket, Delimiter=delimiter, Prefix=prefix)
    common_prefixes = s3_result.get('CommonPrefixes')
    if not common_prefixes:
        return []
    res = [key['Prefix'].split(delimiter)[-2] for key in common_prefixes]
    return res


def list_s3_partitions(s3_location, partition_type='%Y-%m-%d'):
    bucket = s3_location.split('/')[0]
    prefix = '/'.join(s3_location.split('/')[1:])
    if partition_type == '%Y/%m/%d':
        source_list = list_partitions_gi(bucket, prefix)
    elif partition_type == '%Y-%m-%d':
        source_list = list_partitions_dt(bucket, prefix)
    return source_list


def ses_send_plain_email(subject, text, destination,
                         attachments=None,
                         source='no-reply@toolbox-analytics.com'):
    msg = MIMEMultipart()
    msg['Subject'] = subject
    msg['From'] = source
    msg['To'] = destination
    part = MIMEText(text, "plain")

    msg.attach(part)

    if attachments:
        for attachment in attachments:
            part = MIMEBase('application', "octet-stream")
            part.set_payload(open(attachment, "rb").read())
            encoders.encode_base64(part)
            part.add_header('Content-Disposition',
                            f'attachment; filename="{attachment}"')
            msg.attach(part)

    client = boto3.client('ses', region_name='us-east-1')

    client.send_raw_email(
        RawMessage={'Data': msg.as_string()},
        Source=msg['From'],
        Destinations=[
            destination,
            'user1@company.net',
            'user2@company.com'
        ]
    )


# Send email in HTML format but it doesn't send attachments be careful !
def ses_send_html_email(subject, html_msg, destination,
                        EMAIL_DIST_LIST=[], region="us-east-1", txt_msg="TEXT MSG"):
    RECIPIENT = destination.lower()
    SENDER = "No-Reply <no-reply@company.com>"

    # Create a new SES resource and specify a region.
    client = boto3.client('ses', region_name=region)

    try:
        response = client.send_email(
            Destination={
                'ToAddresses': [
                    RECIPIENT,
                ],
                # 'CcAddresses': [
                #    CC_RCPT,
                # ],
                'BccAddresses': [
                    *EMAIL_DIST_LIST,
                ]
            },
            Message={
                'Body': {
                    'Html': {
                        'Charset': "UTF-8",
                        'Data': html_msg,
                    },
                    'Text': {
                        'Charset': "UTF-8",
                        'Data': txt_msg,
                    },
                },
                'Subject': {
                    'Charset': "UTF-8",
                    'Data': subject,
                },
            },
            Source=SENDER
        )
    except ClientError as e:
        print(f"Error sending email to:{destination} - {e.response['Error']['Message']}")
    else:
        print(f"Email sent to:{destination}! Message ID:{response['MessageId']}")


def get_latest_dt(s3_path,
                  partition_regex=r"dt=\d{4}-\d{2}-\d{2}",
                  dt_format="dt=%Y-%m-%d"):
    """
    AWS helper function that returns the latest date partition in the s3 path you prove id.
    :param s3_path: S3 path with partitions under it (e.g. s3://bucket/prefix/)
    :param partition_regex: Date partition regex to match.
    :param dt_format: Date format to parse.
    :return: String of latest date partition in dt_format.
    """

    s3_path = os.path.join(s3_path, "")

    try:
        aws_ls_response = run(
            """aws s3 ls {s3_path}""".format(
                s3_path=s3_path.replace("s3a://", "s3://")),
            shell=True,
            stdout=PIPE,
            check=True
        ).stdout.decode()

        dates = re.findall(partition_regex, aws_ls_response)

        if len(dates) > 0:
            dates = [datetime.strptime(date_str, dt_format)
                     for date_str in dates]
            return datetime.strftime(max(dates), dt_format)
        else:
            raise ValueError(
                "Did not find any dates that match partition_regex: "
                "{pr} for path: {s3_path}".format(
                    pr=partition_regex, s3_path=s3_path
                )
            )

    except CalledProcessError as e:
        raise ValueError(
            "Not able to access: {s3_path}".format(s3_path=s3_path)
        )

