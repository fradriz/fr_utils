{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "\n",
    "## Big Data Myths\n",
    "\n",
    "* Since Spark is faster use it always over Hadoop MapReduce\n",
    "* Add more nodes to the cluster to speed up the process.\n",
    "* Since Spark was written in Scala, then Scala is always faster than Python.\n",
    " + https://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python\n",
    " + https://stackoverflow.com/questions/52713466/in-theory-scala-is-faster-than-python-for-apache-spark-in-practice-it-is-not\n",
    "\n",
    "At the end of this course, we will challenge and sometimes discard above sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The two most widely used big data distributed frameworks are *Hadoop MapReduce* and *Apache Spark*.\n",
    "\n",
    "* Hadoop is a distributed filesystem (HDFS) while [Apache Spark](https://spark.apache.org/?utm_source=xp&utm_medium=blog&utm_campaign=content) needs a filesystem to work on.\n",
    " + In a matter of fact, Spark is actually designed to run on top of Hadoop.\n",
    "\n",
    "\n",
    "## [What is Hadoop MapReduce?](https://www.xplenty.com/blog/apache-spark-vs-hadoop-mapreduce/)\n",
    "\n",
    "The MapReduce paradigm consists of two sequential tasks: Map and Reduce. \n",
    "* Map filters and sorts data while converting it into key-value pairs. \n",
    "* Reduce then takes this input and reduces its size by performing some kind of summary operation over the dataset.\n",
    "\n",
    "MapReduce can drastically speed up big data tasks by breaking down large datasets and processing them in parallel.\n",
    "\n",
    "<font color=red>__Important__: to take advantage of the above __data must be splittable__. Take this into account when analyzing the data and the needed type of work and when you wonder why _more nodes does not mean more processing speed_. __No all the data meet that condition__.</font>\n",
    "\n",
    "### The Differences Between Spark and MapReduce\n",
    "The main differences between Apache Spark and Hadoop MapReduce are:\n",
    "* Performance\n",
    "* Ease of use\n",
    "* Data processing\n",
    "* Security\n",
    "\n",
    "#### Spark vs MapReduce: Performance\n",
    "Apache Spark processes data in RAM, while Hadoop MapReduce persists data back to the disk after a map or reduce action. In theory, then, Spark should outperform Hadoop MapReduce.\n",
    "\n",
    "__*Spark needs a lot of memory*__: Much like standard databases, Spark loads a process into memory and keeps it there until further notice for the sake of caching. Consider that: \n",
    "* If you run Spark on Hadoop YARN with other resource-demanding services, \n",
    "* .. or if the data is too big to fit entirely into memory..\n",
    "\n",
    "__=> then Spark could suffer major performance degradations__\n",
    "\n",
    "MapReduce, on the other hand, kills its processes as soon as a job is done, so it can easily run alongside other services with minor performance differences.\n",
    "\n",
    "* Iterative computations that need to pass over the same data many times: *Use Spark*\n",
    "* one-pass ETL-like jobs —for example, data transformation or data integration: *that's exactly what MapReduce was designed for*.\n",
    "\n",
    "__Bottom line__: Spark performs better when all the data fits in memory, *especially on dedicated clusters.* Hadoop MapReduce is designed for data that doesn’t fit in memory, and can run well alongside other services.\n",
    "\n",
    "#### Spark vs MapReduce: Ease of Use\n",
    "Spark has pre-built APIs for Java, Scala and Python, and also includes Spark SQL. Thanks to Spark’s simple building blocks, it’s easy to write user-defined functions. Spark even includes an interactive mode for running commands with immediate feedback.\n",
    "\n",
    "MapReduce is written in Java and is not easy to program directly. Althouth, there some projects that makes it easier:\n",
    "* Apache Pig (it requires some time to learn the syntax)\n",
    "* Apache Hive adds SQL\n",
    "* Projects like Apache Impala and Apache Tez want to bring full interactive querying to Hadoop.\n",
    "\n",
    "__Bottom line:__ Spark is easier to program and includes an interactive mode. Hadoop MapReduce is more difficult to program, but several tools are available to make it easier.\n",
    "\n",
    "#### Spark vs MapReduce: Data Processing\n",
    "* **Spark can do more than plain data processing**: it can also process graphs, including MLlib machine learning library, can do real-time processing as well as batch processing. \n",
    "\n",
    "* **Hadoop MapReduce is great for batch processing**: If you want a real-time option you’ll need to use another platform like Impala or Apache Storm, and for graph processing you can use Apache Giraph. MapReduce used to have Apache Mahout for machine learning, but it's since been ditched in favor of Spark and H2O.\n",
    "\n",
    "__Bottom line:__ Spark is the Swiss army knife of data processing, while Hadoop MapReduce is the commando knife of batch processing.\n",
    "\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "* Apache Spark is potentially 100 times faster than Hadoop MapReduce...when data fits in memory space.\n",
    "* Apache Spark isn’t tied to Hadoop’s two-stage map/reduce paradigm.\n",
    "* Hadoop is more cost effective processing massive data sets.\n",
    "* Apache Spark is now more popular that Hadoop MapReduce. [Check google Trends](https://trends.google.com/trends/explore?cat=5&date=2008-06-07%202020-07-07&geo=US&q=hadoop,spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
