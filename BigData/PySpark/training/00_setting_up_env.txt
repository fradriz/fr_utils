# Creating a virtualenv and installing pyspark from scratch
# Installing pyspark: https://medium.com/@ashok.tankala/run-your-first-spark-program-using-pyspark-and-jupyter-notebook-3b1281765169

user$ virtualenv --version
virtualenv 20.0.15 from /usr/local/lib/python3.7/site-packages/virtualenv/__init__.py

user$ pwd
/Users/facradri/PycharmProjects/virtualenvs

user$ virtualenv env_pyspark/
created virtual environment CPython3.7.5.final.0-64 in 375ms
  creator CPython3Posix(dest=/Users/facradri/PycharmProjects/virtualenvs/env_pyspark, clear=False, global=False)
  seeder FromAppData(download=False, pip=latest, setuptools=latest, wheel=latest, via=copy, app_data_dir=/Users/facradri/Library/Application Support/virtualenv/seed-app-data/v1.0.1)
  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator


user$ source env_pyspark/bin/activate

source /Users/facradri/PycharmProjects/virtualenvs/env_pyspark/bin/activate

(env_pyspark)  user$ which python
/Users/facradri/PycharmProjects/virtualenvs/env_pyspark/bin/python


(env_pyspark)  user$ pip list
Package    Version
---------- -------
pip        20.0.2 
setuptools 46.1.3 
wheel      0.34.2 
WARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.
You should consider upgrading via the '/Users/facradri/PycharmProjects/virtualenvs/env_pyspark/bin/python -m pip install --upgrade pip' command.

(env_pyspark)  user$ pip install pyspark
Collecting pyspark
  Downloading pyspark-3.0.0.tar.gz (204.7 MB)
     |████████████████████████████████| 204.7 MB 955 kB/s 
Collecting py4j==0.10.9
  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)
     |████████████████████████████████| 198 kB 923 kB/s 
Building wheels for collected packages: pyspark
  Building wheel for pyspark (setup.py) ... done
  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=3004c7b119e0021da88c3cbcc70b3ce086957b92df1e255c0b3b87e451998c7b
  Stored in directory: /Users/facradri/Library/Caches/pip/wheels/4e/c5/36/aef1bb711963a619063119cc032176106827a129c0be20e301
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9 pyspark-3.0.0

(env_pyspark)  user$ pip list
Package    Version
---------- -------
pip        20.0.2 
py4j       0.10.9 
pyspark    3.0.0  
setuptools 46.1.3 
wheel      0.34.2 

(env_pyspark)  user$ pip install jupyter
Collecting jupyter
  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)
...
     |████████████████████████████████| 100 kB 1.3 MB/s 
Collecting pyparsing>=2.0.2
  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
     |████████████████████████████████| 67 kB 1.3 MB/s 
Collecting zipp>=0.5
  Using cached zipp-3.1.0-py3-none-any.whl (4.9 kB)
Building wheels for collected packages: tornado, pyrsistent
...

(env_pyspark)  user$ pip list
Package            Version
------------------ -------
appnope            0.1.0  
attrs              19.3.0 
backcall           0.2.0  
bleach             3.1.5  
...
zipp               3.1.0  
WARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.
You should consider upgrading via the '/Users/facradri/PycharmProjects/virtualenvs/env_pyspark/bin/python -m pip install --upgrade pip' command.

# Now, where is spark ?
(env_pyspark)  user$ pip show pyspark
Name: pyspark
Version: 3.0.0
Summary: Apache Spark Python API
Home-page: https://github.com/apache/spark/tree/master/python
Author: Spark Developers
Author-email: dev@spark.apache.org
License: http://www.apache.org/licenses/LICENSE-2.0
Location: /Users/facradri/PycharmProjects/virtualenvs/env_pyspark/lib/python3.7/site-packages
Requires: py4j
Required-by: 

# SETTING UP ENV VARIABLES ..
# OBS: these changes are ephemeral, to make them permanent need to add it in 'vi ~/.bash_profile' but it'll affect but not do it in your profile
# from above 'location'

export SPARK_HOME=/Users/facradri/PycharmProjects/virtualenvs/env_pyspark/lib/python3.7/site-packages/pyspark
export PATH=$SPARK_HOME/bin:$PATH  # To run pyspark shell this line too.
export PYSPSPARK_PYTHON=python3

# To run a notebook just executing 'pyspark' ..
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

# Check how are the env vars..
$ printenv


# In our case, we want to run through Jupyter, so we have to find the spark based on our SPARK_HOME.
# We need to install findspark pacakge.
pip install findspark

