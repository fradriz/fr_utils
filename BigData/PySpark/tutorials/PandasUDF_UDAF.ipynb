{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas UDFs and UDAFs in pyspark\n",
    "Links: \n",
    "* https://spark.apache.org/docs/2.4.0/sql-pyspark-pandas-with-arrow.html\n",
    "* https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark/\n",
    "* https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "* Funcionamiento de Spark + UDAFs con Pandas https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/\n",
    "* https://stackoverflow.com/questions/55506698/writing-custom-udaf-in-pyspark\n",
    "* https://stackoverflow.com/questions/40006395/applying-udfs-on-groupeddata-in-pyspark-with-functioning-python-example/47497815#47497815\n",
    "* https://stackoverflow.com/questions/32100973/how-to-define-and-use-a-user-defined-aggregate-function-in-spark-sql\n",
    "\n",
    "* versions: https://stackoverflow.com/questions/51713705/python-pandas-udf-spark-error\n",
    "\n",
    "---\n",
    "Obs: Is important to have the right combination of version with these packages:\n",
    "\n",
    "* numpy: 1.14.5\n",
    "* pyarrow: 0.10.0\n",
    "* pandas: 0.24.2\n",
    "---\n",
    "\n",
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## UDF - Grouped Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use groupBy().apply(), the user needs to define the following:\n",
    "\n",
    "* A *Python function* that defines the computation for each group.\n",
    "* A *StructType object* or a string that defines the **schema** of the output DataFrame.\n",
    "\n",
    "*Important: Note that all data for a group will be loaded into memory before the function is applied*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n",
      "CPU times: user 16.3 ms, sys: 4.13 ms, total: 20.4 ms\n",
      "Wall time: 966 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## UDAF - Grouped Aggregate\n",
    "\n",
    "*Important: Note that this type of UDF does not support partial aggregation and all data for a group or window will be loaded into memory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n",
      "CPU times: user 7.82 ms, sys: 12.2 ms, total: 20 ms\n",
      "Wall time: 939 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def mean_udf(v):\n",
    "    return v.mean()\n",
    "\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window \\\n",
    "    .partitionBy('id') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Arrow Batch Size\n",
    "\n",
    "Data partitions in Spark are converted into Arrow record batches, which can temporarily lead to high memory usage in the JVM. *To avoid possible out of memory exceptions*, the size of the Arrow record batches can be adjusted by setting the conf **“spark.sql.execution.arrow.maxRecordsPerBatch”** to an integer that will determine the maximum number of rows for each batch. \n",
    "\n",
    "The default value is 10,000 records per batch. If the number of columns is large, the value should be adjusted accordingly. Using this limit, each data partition will be made into 1 or more record batches for processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
