{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding repartitions in Spark\n",
    "Link: https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\n",
    "\n",
    "First: reading data that has 401 parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://spineds-testing/vendor-comscore/web-panel/traffic/dt=2020-04-08/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592,707,174\n"
     ]
    }
   ],
   "source": [
    "p = \"s3a://spineds-testing/vendor-comscore/web-panel/traffic/dt=2020-04-08/\"\n",
    "#df.unpersist()\n",
    "df = spark.read.parquet(p)\n",
    "\n",
    "#.limit(1000000).cache()\n",
    "\n",
    "print(f\"{df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many partitions is the data divided ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a portion of the data (think sample works similiar to limit).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.sample(True, 1/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows do we have now ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,951\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df2.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many partitions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow ! \n",
    "* Original data has: 592,707,174 rows and 414 paritions => 1,431,659 rows / partition\n",
    "* Sample data has: 5,951 rows in 414 partitions => 14 rows per partition !!!\n",
    "\n",
    "Someting is no good here !\n",
    "\n",
    "Let's see what happen if we store this data without any repartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write \\\n",
    "      .mode('overwrite') \\\n",
    "      .parquet(\"/home/user/repa/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/user/repa/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 user user 7.6K Apr 24 21:43 part-00409-e3e31258-fa2c-4633-ae5d-db1a5cfcfc7d-c000.snappy.parquet\n",
      "-rw-r--r-- 1 user user 7.8K Apr 24 21:43 part-00410-e3e31258-fa2c-4633-ae5d-db1a5cfcfc7d-c000.snappy.parquet\n",
      "-rw-r--r-- 1 user user 6.9K Apr 24 21:43 part-00411-e3e31258-fa2c-4633-ae5d-db1a5cfcfc7d-c000.snappy.parquet\n",
      "-rw-r--r-- 1 user user 6.9K Apr 24 21:43 part-00412-e3e31258-fa2c-4633-ae5d-db1a5cfcfc7d-c000.snappy.parquet\n",
      "-rw-r--r-- 1 user user 5.2K Apr 24 21:43 part-00413-e3e31258-fa2c-4633-ae5d-db1a5cfcfc7d-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/user/repa/ | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have like 416 files with a size of 128 MB. It doesn't make sense.\n",
    "\n",
    "Let's see what's in one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "local_file = \"/home/user/repa/part-00001-e3e31258-fa2c-4633-ae5d-db1a5cfcfc7d-c000.snappy.parquet\"\n",
    "df3 = spark.read.parquet(local_file)\n",
    "print(f\"{df3.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is highly inefficient ! \n",
    "\n",
    "Since we have less than 6000 rows, then we can put all the data just in one partition.\n",
    "\n",
    "Going from 414 partitions to 1. We can use coalsce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write \\\n",
    "      .mode('overwrite') \\\n",
    "      .parquet(\"/home/user/repa2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /home/user/repa2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
