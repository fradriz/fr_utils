{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/facradri/Dropbox/Tech/apps/Python/PySpark/pySparkTutorial\r\n",
      "total 9632\r\n",
      "-rwxr-xr-x@  1 facradri  LL\\Domain Users       74 Aug 25  2018 \u001b[31mREADME.md\u001b[m\u001b[m\r\n",
      "drwxr-x---@ 25 facradri  LL\\Domain Users      800 Jan 29  2016 \u001b[34mml-100k\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x@  1 facradri  LL\\Domain Users  4924029 Aug 25  2018 \u001b[31mml-100k.zip\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!pwd;ls -l Data-ML-100k--master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\r\n",
      "186\t302\t3\t891717742\r\n",
      "22\t377\t1\t878887116\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 3 Data-ML-100k--master/ml-100k/u.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+\n",
      "|_c0|_c1|_c2|      _c3|\n",
      "+---+---+---+---------+\n",
      "|196|242|  3|881250949|\n",
      "|186|302|  3|891717742|\n",
      "| 22|377|  1|878887116|\n",
      "+---+---+---+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File 'data' is a CSV that uses TAB as separator and does not have header\n",
    "ratings = spark.read.load(\"Data-ML-100k--master/ml-100k/u.data\",format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")\n",
    "ratings.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Column names / Add header\n",
    "Good functionality. Always required. Donâ€™t forget the * in front of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    196|     242|     3|     881250949|\n",
      "|    186|     302|     3|     891717742|\n",
      "|     22|     377|     1|     878887116|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])\n",
    "ratings.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#print(ratings.count()) #Row Count\n",
    "print(len(ratings.columns)) #Column Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|movie_id|\n",
      "+-------+--------+\n",
      "|    196|     242|\n",
      "|    186|     302|\n",
      "|     22|     377|\n",
      "+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.select('user_id','movie_id').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "Filter a dataframe using multiple conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    253|     465|     5|     891628467|\n",
      "|    253|     510|     5|     891628416|\n",
      "|    253|     183|     5|     891628341|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.filter((ratings.rating==5) & (ratings.user_id==253)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "We can use groupby function with a spark dataframe too. \n",
    "\n",
    "Pretty much same as a pandas groupby with the exception that you will need to import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+\n",
      "|user_id|count(user_id)|       avg(rating)|\n",
      "+-------+--------------+------------------+\n",
      "|    148|            65|               4.0|\n",
      "|    463|           133|2.8646616541353382|\n",
      "|    471|            31|3.3870967741935485|\n",
      "+-------+--------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "ratings.groupBy(\"user_id\").agg(F.count(\"user_id\"),F.mean(\"rating\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|      1|      33|     4|     878542699|\n",
      "|      1|     202|     5|     875072442|\n",
      "|      1|     160|     4|     875072547|\n",
      "|      1|      61|     4|     878542420|\n",
      "|      1|     189|     3|     888732928|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.sort(\"user_id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    943|     570|     1|     888640125|\n",
      "|    943|     186|     5|     888639478|\n",
      "|    943|     232|     4|     888639867|\n",
      "|    943|      58|     4|     888639118|\n",
      "|    943|    1067|     2|     875501756|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# descending Sort\n",
    "from pyspark.sql import functions as F\n",
    "ratings.sort(F.desc(\"user_id\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins/Merging with Spark Dataframes\n",
    "\n",
    "I was not able to find a pandas equivalent of merge with Spark DataFrames but we can use SQL with dataframes and thus we can merge dataframes using SQL.\n",
    "\n",
    "Let us try to run some SQL on Ratings.\n",
    "\n",
    "We first register the ratings df to a temporary table ratings_table on which we can run sql operations.\n",
    "\n",
    "As you can see the result of the SQL select statement is again a Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.registerTempTable('ratings_table')\n",
    "\n",
    "#newDF = spark.sql('select * from ratings_table where rating>4')\n",
    "\n",
    "#newDF.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
