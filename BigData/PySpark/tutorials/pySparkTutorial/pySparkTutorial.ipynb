{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tutorial\n",
    "Link: \n",
    "* Blog post: https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a\n",
    "* GitHub: https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a38fcdf1fb47:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a38fcdf1fb47:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f182c0a8240>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Programming in Python\n",
    "Look for this book in Amazon:\n",
    "* *Functional Python Programming: Discover the power of functional programming, generator functions, lazy evaluation, the built-in itertools library, and monads, 2nd Edition*\n",
    "\n",
    "### Map\n",
    "Say you want to apply some function to every element in a list.\n",
    "\n",
    "You can do this by simply using a for loop but python lambda functions let you do this in a single line in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want to square each term in my_list.\n",
    "squared_list = map(lambda x:x**2,my_list)\n",
    "\n",
    "print(list(squared_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above example, you could think of map as a function which takes two arguments — A function and a list.\n",
    "\n",
    "It then applies the function to every element of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "The other function that is used extensively is the `filter` function. This function takes two arguments — A condition and the list to filter.\n",
    "\n",
    "If you want to filter your list using some condition you use filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want only the even numbers in my list.\n",
    "filtered_list = filter(lambda x:x%2==0,my_list)\n",
    "\n",
    "print(list(filtered_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "This function takes two arguments:\n",
    "* a function to reduce that takes two arguments\n",
    "* and a list over which the reduce function is to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# In python3 reduce needs an import\n",
    "import functools as f\n",
    "my_list = [1,2,3,4,5]\n",
    "\n",
    "# Lets say I want to sum all elements in my list.\n",
    "sum_list = f.reduce(lambda x,y:x+y,my_list)\n",
    "\n",
    "print(sum_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A condition on the lambda function we use in reduce is that it must be:\n",
    "* **commutative** that is a + b = b + a and\n",
    "* **associative** that is (a + b) + c == a + (b + c).\n",
    "\n",
    "In the above case, we used sum which is commutative as well as associative. Other functions that we could have used: max, min, * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Spark\n",
    "Spark actually consists of two things a *driver* and *workers*.\n",
    "\n",
    "Workers normally do all the work and the driver makes them do that work.\n",
    "\n",
    "### RDD\n",
    "An RDD(Resilient Distributed Dataset) is a parallelized data structure that gets distributed across the worker nodes. \n",
    "\n",
    "**They are the basic units of Spark programming.**\n",
    "\n",
    "For example, given this line `lines = sc.textFile(\"/FileStore/tables/shakespeare.txt\")`\n",
    "\n",
    "We took a text file and distributed it across worker nodes so that they can work on it in parallel. \n",
    "\n",
    "We could also parallelize lists using the function sc.parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "new_rdd = sc.parallelize(data,4)\n",
    "new_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, we can do two different types of operations on RDD: Transformations and Actions.\n",
    "\n",
    "* **Transformations**: Create new datasets from existing RDDs\n",
    "* **Actions**: Mechanism to get results out of Spark\n",
    "\n",
    "## Transformation Basics\n",
    "So let us say you have got your data in the form of an RDD. You want to do some transformations on the data now.\n",
    "\n",
    "You may want to filter, apply some function, etc.\n",
    "\n",
    "In Spark, this is done using Transformation functions.\n",
    "\n",
    "Spark provides many transformation functions. You can see a comprehensive list [here](http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). \n",
    "\n",
    "Some of the main ones are:\n",
    "\n",
    "### Map\n",
    "Applies a given function to an RDD.\n",
    "\n",
    "Note that the syntax is a little bit different from Python, but it necessarily does the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List\n",
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "# List to RDD\n",
    "rdd = sc.parallelize(data,4)\n",
    "# Apply map\n",
    "squared_rdd = rdd.map(lambda x:x**2)\n",
    "# Show it\n",
    "squared_rdd.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "filtered_rdd = rdd.filter(lambda x:x%2==0)\n",
    "filtered_rdd.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatmap\n",
    "Similar to `map`, but ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 8], [3, 27], [4, 64], [5, 125]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [2,3,4,5]\n",
    "rdd = sc.parallelize(data,4)\n",
    "# La salida viene en una lista de listas [numero, numero^3] (puede ser tuplas tmb)\n",
    "flat_rdd = rdd.map(lambda x:[x,x**3])\n",
    "flat_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 3, 27, 4, 64, 5, 125]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [2,3,4,5]\n",
    "rdd = sc.parallelize(data,4)\n",
    "# La salida viene en una lista de numeros contiguos: numero, numero^3\n",
    "flat_rdd = rdd.flatMap(lambda x:[x,x**3])\n",
    "flat_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceByKey\n",
    "(Con RDDs, *reduce* es una acción !!!)\n",
    "\n",
    "Let's assume we have a data in which we have a product, its category, and its selling price. We can still parallelize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),\n",
    "        ('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our RDD rdd holds tuples.\n",
    "\n",
    "Now we want to find out the total sum of revenue that we got from each category.\n",
    "\n",
    "To do that we have to transform our rdd to a pair rdd so that it only contains key-value pairs/tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 200),\n",
       " ('Fruit', 24),\n",
       " ('Fruit', 56),\n",
       " ('Vegetable', 103),\n",
       " ('Vegetable', 34)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_price_rdd = rdd.map(lambda x: (x[1],x[2]))\n",
    "category_price_rdd.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our category_price_rdd contains the product category and the price at which the product sold.\n",
    "\n",
    "Now we want to reduce on the key category and sum the prices. We can do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 280), ('Vegetable', 137)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y)\n",
    "category_total_price_rdd.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey\n",
    "Similar to reduceByKey but does not reduces just puts all the elements in an iterator. \n",
    "\n",
    "For example, if we wanted to keep as key the category and as the value all the products we would use this function.\n",
    "\n",
    "Let us again use map to get data in the required form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 'Apple'),\n",
       " ('Fruit', 'Banana'),\n",
       " ('Fruit', 'Tomato'),\n",
       " ('Vegetable', 'Potato'),\n",
       " ('Vegetable', 'Carrot')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),\n",
    "        ('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data,4)\n",
    "category_product_rdd = rdd.map(lambda x: (x[1],x[0]))\n",
    "category_product_rdd.take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit ['Apple', 'Banana', 'Tomato']\n",
      "Vegetable ['Potato', 'Carrot']\n"
     ]
    }
   ],
   "source": [
    "grouped_products_by_category_rdd = category_product_rdd.groupByKey()\n",
    "\n",
    "findata = grouped_products_by_category_rdd.take(30)\n",
    "\n",
    "for data in findata:\n",
    "    print(data[0],list(data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Basics\n",
    "\n",
    "You have filtered your data, mapped some functions on it. Done your computation.\n",
    "\n",
    "Now you want to get the data on your local machine or save it to a file or show the results in the form of some graphs in excel or any visualization tool. **You will need actions for that**. \n",
    "\n",
    "A comprehensive list of actions is provided [here](http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "\n",
    "Some of the most common actions are:\n",
    "\n",
    "### collect\n",
    "(no funciona local, si funciona en toolbox )\n",
    "It takes the *whole RDD* and brings it back to the driver program.\n",
    "\n",
    "### take\n",
    "(igual al collect, pero solo toma los primeros n elementos)\n",
    "Sometimes you will need to see what your RDD contains without getting all the elements in memory itself. take returns a list with the first n elements of the RDD.\n",
    "\n",
    "### reduce\n",
    "(con RDDs, *reduceByKey* es una transformación !!!)\n",
    "Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.reduce(lambda x,y : x+y)\n",
    "# Esto da 15 pero en el notebook local me da error (problema de config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeOrdered:\n",
    "takeOrdered returns the first n elements of the RDD using either their natural order or a custom comparator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 12, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([5,3,12,23])\n",
    "# descending order\n",
    "rdd.takeOrdered(3,lambda s:-1*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 344), (3, 34), (23, 29)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)])\n",
    "# descending order\n",
    "rdd.takeOrdered(3,lambda s:-1*s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding The WordCount Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parque: 8\n",
      "Pedro: 1\n",
      "y: 6\n",
      "hoy: 1\n",
      "del: 3\n",
      "que: 2\n",
      "hay: 1\n",
      "junto: 1\n",
      "mi: 3\n",
      "divierto: 1\n"
     ]
    }
   ],
   "source": [
    "# Distribute the data - Create a RDD \n",
    "lines = sc.textFile(\"ej.txt\")\n",
    "\n",
    "# Create a list with all words, Create tuple (word,1), reduce by key i.e. the word\n",
    "counts = (lines.flatMap(lambda x: x.split(' '))          \n",
    "                  .map(lambda x: (x, 1))                 \n",
    "                  .reduceByKey(lambda x,y : x + y))\n",
    "\n",
    "# get the output on local\n",
    "output = counts.take(10)                                 \n",
    "\n",
    "# print output\n",
    "for (word, count) in output:                             \n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2da linea\n",
    "Analizamos que hacen los map, flatmap y reduceBykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El', 'parque', 'Me']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si se aplica map, cada linea la hace una lista separada, para tener todas las palabras juntas es mejor usar flatMap\n",
    "lines.flatMap(lambda x: x.split(' ')).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El', 'parque', 'Me', 'llamo', 'Pedro']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si se aplica map, cada linea la hace una lista separada, para tener todas las palabras juntas es mejor usar flatMap\n",
    "# lines.map(lambda x: x.split(' ')).take(3)\n",
    "# flatMap pasa todas las palabras a una lista (es igual a hacer lines.split(' ') en python)\n",
    "a = lines.flatMap(lambda x: x.split(' '))\n",
    "a.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 1), ('parque', 1), ('Me', 1), ('llamo', 1), ('Pedro', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A c/palabra la ubico en una tupla (palabra, 1)\n",
    "b = a.map(lambda x: (x, 1)) \n",
    "b.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parque', 8),\n",
       " ('Pedro', 1),\n",
       " ('y', 6),\n",
       " ('hoy', 1),\n",
       " ('del', 3),\n",
       " ('que', 2),\n",
       " ('hay', 1),\n",
       " ('junto', 1),\n",
       " ('mi', 3),\n",
       " ('divierto', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esto agrupa/cuenta por palabra\n",
    "c = b.reduceByKey(lambda x,y : x + y)\n",
    "c.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark in Action with Example\n",
    "\n",
    "See *02_MoviesRDDs.ipynb*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
