{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Home Exam - company\n",
    "by Facundo Radrizzani\n",
    "\n",
    "## Introduction\n",
    "\n",
    "I've solved the three excercises with pySpark using [Databricks notebooks](https://databricks.com/). I am delivering three notebooks in HTML format:\n",
    "\n",
    "* 01_IdentityResolution.html\n",
    "* 02_SegmentStandarization.html\n",
    "* 03_JoinTaxonomy.html\n",
    "\n",
    "I've used default settings in the notebooks:\n",
    "* Python version 3.3.5 \n",
    "* Apache Spark 2.4.0\n",
    "\n",
    "*SparkSession* comes ready by default in Databricks. Though, usually is necessary to configure it. This is an example as how to define a basic *'SparkSession'* and connect it to Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Take Home Exam\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization \n",
    "### Broadcast Hash Joins\n",
    "\n",
    "As I mention in the solution notebooks, dataframes in the three excercises have very different sizes. When some conditions are met, it might be useful to use *Broadcast Hash Join* instead of just *Hash Join*.\n",
    "\n",
    "In a distributed cluster, broacast the small table into all the nodes will lower the amount of traffic coming in and out from the network and thus, speeding up the performance.\n",
    "\n",
    "Sometimes cluster use it automatically without the need of any manual intervention. As I check it, in some systems, this is not always the case. To see what is happening in the join we can use *explain* or benchmark the queries.\n",
    "\n",
    "Neccesary conditions to use Broadcast joins is that the **small (broadcasted) table should fit in memory of every node of the cluster**. \n",
    "\n",
    "System has one value threshold, if the table is smaller of the threshold then the system could use broadcast joins automatically -as mentioned before, this is not always true-. \n",
    "Use **spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")** to check it. In case of need, is possible to change this value.\n",
    "\n",
    "## Persist the data\n",
    "\n",
    "As it wasn't needed for the exam, I haven't saved the data in a file (parquet, csv or any other format) for further usage, just commented about it in one of the exercises."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
